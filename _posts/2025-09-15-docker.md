# Docker

Explanation of the architecture of Docker and how to create a running container of your app and your PostgreSQL database.

Mental picture: with Docker you put your application in an environment that you can control extensively. To create this environment, you either create an image (write a Dockerfile, then run it) or you use an existing image. 'Environment' in this context means the filesystem, which is the directory structure plus all its contents, as seen from inside the container.

## Docker architecture

Everything you need for Docker can be pulled in by installing Docker Desktop. It includes Docker cli, so you can run commands from a shell, and also the so-called daemon, or dockerd, that manages the containers. These two communicate with a REST API, by default over UNIX sockets but you can configure it to communivcate over a network interface (less safe but suitable if client and daemon live on different machines).

The third important element in Docker is the registry, where images are centrally stored. The default registry is Docker Hub but you can create your own, which you might typically do in an organization. 

The other relevant elements of Docker are Docker Compose, a tool to run and define multi-container applications, and Docker Swarm, an orchestration service. The latter is an alternative for Kubernetes.

### Underlying Linux and other software

Docker relies on three Linux elements that predated Docker, namely [chroot](https://man7.org/linux/man-pages/man1/chroot.1.html), [cgroups](https://man7.org/linux/man-pages/man7/cgroups.7.html) and [namespaces](https://man7.org/linux/man-pages/man7/namespaces.7.html). The existence of these kernel features shows that the creation of isolated environments, where applications cannot see the underlying OS filesystem or its environment variables, was recognized as something important early on. 

Apart from these Linux kernel features, Docker also relies on [containerd](https://containerd.io/) as the basis for individual containers. Containerd is an open source tool that provides a layer of abstraction upon the basic Linux kernel features and specializes in the running of containers. It gets its instructions from dockerd. Between the kernel features chroot, cgroups and namespaces and containerd lives another creature named runc. It is not part of Linux and written in Go. Both containerd and runc play a large role in the cloud and in kubernetes, and are highly relevant outside of the Docker context.

### A container is a process, an image is a file

While the term container suggests something similar to a file, or at least something that will exist even if the machine is switched off, it isn't. A container is a process based upon an image, which actually is a (large) file. Once you have built a container on a machine you can start and stop it and it won't have to rebuild or rerun every time, but migrating it means that it needs another 'docker run' command on the other system. 

### Image as a layered construct

An image is built following a recipe found in a dockerfile that you, or someone else, has composed. A dockerfile contains a list of instructions, whereby every instruction affects the resulting filesystem and its contents (the 'environment'). While as a user you are only interested in the final filesystem, the image actually stores multiple filesystems, one for every instruction. Each is called a layer. 

The rationale for layering is that these layers can be reused which saves time. Say that two different and unrelated images each install some application, resulting in all sorts of new files and folders added to the environment, then only the first install has to take place. The second image can use the layer that is the result of the first installation.

I asked ChatGPT what this means if you have instructions that delete content. Say I add an instruction that copies 100Mb of files to the environment/filesystem, and a next instruction deletes them. The final result is the same as if no instructions had taken place, but the image will still contain a layer containing all the copied files. The image will have those 100Mb stored as part of the first layer.

One consequence of this is that if you want to limit the size of the final image and thus want to clean up during the build process, you cannot delete the mess you made in some instruction ("copy all sorts of temporary installation files to my environment") in a next instruction ("now delete the garbage"). The workaround is in the RUN instruction, that allows you to chain commands with `&&`. As both the mess making and cleaning happens within the same instruction, no residue will persist in the final image.

## Basic processes

Docker can be fairly well understood when you know the following commands:

- `docker run`
- `docker build`
- `docker pull`

### docker run

The run command presupposes the existence of an image. Any run command requires an image as argument, more specifically the last argument unless that image requires some arguments itself. This image can be an image from Docker Hub or one that you got from your colleague. The typical form is:

```
docker run [OPTIONS] IMAGE [COMMAND] [ARG...] 
```

Take this example, in which a self-generated image is used ('myapp'):

```
docker run -d -p 8080:8080 -v /home/user/data:/app/data myapp:latest
```

Or this one, that starts up a container with a PostgreSQL database:

```
docker run --name my-postgres \
  -e POSTGRES_USER=testuser \
  -e POSTGRES_PASSWORD=testpass \
  -e POSTGRES_DB=testdb \
  -v pgdata:/var/lib/postgresql/data \
  -p 5432:5432 \
  -d postgres:16
```

Note that both these run commands end with the name of an image, not followed by some command or arguments. Most often this is sufficient but there are occassions where you want to provide some command with arguments and where the image accomodates this:

```
docker run --rm python:3.11 python -c "print(3 * 7)"  // runs Python and prints 21

docker run --rm postgres:17 postgres --version  // prints the postgres version

docker run --rm alpine:3.19 ls -l /bin  // prints a directory listing of /bin inside the container
```

#### Relevant option flags

A complete overview of flags can be found [here](https://docs.docker.com/reference/cli/docker/container/run/):

|flag|meaning|
|----|----|
|--name|Give the container a name|
|-d|Container will run in background. Not connected to some value|
|-p|Publish a container's port to the host. First number is port within Docker, second is the port for external access|
|-v|Bind a mount volume. For persistence, connects path within container to path in the underlying OS|
|-e|Set environment variable|
|--rm|Automatically remove container and its associated anonymous volumes when it exits|

The last argument is the name of the image in both cases here. 

The reason you want the -v flag is when the data must be safely stored, or when data from the underlying OS must be used. The latter seems tricky to me, as you do not want the container to be dependent on local stuff. But during development there might be use for it. What you definitely want is to store database data persistently.

The environment variables can be set, which is useful in combination with a Spring application.properties files that refers to specific environment variables for database credentials. 

### docker build

You can pick an existing image or create one yourself, whereby you use an existing image as startpoint. To build an image you must create a dockerfile first.  An example of a dockerfile:

```
FROM eclipse-temurin:17-jdk
WORKDIR /app
COPY target/myapp.jar app.jar
EXPOSE 8080
CMD ["java", "-jar", "app.jar"]
```

This example creates a container with a Java app in it. It builds on top if the image specified under FROM. The WORKDIR value 

[Here](https://docs.docker.com/reference/dockerfile/) you find reference with the possible values you can set.
